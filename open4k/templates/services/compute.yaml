#apiVersion: lcm.mirantis.com/v1alpha1
#kind: HelmBundle

{%- set service = 'nova' %}
{%- set components_with_dedicated_messaging = spec.get('features', {}).get('messaging', {}).get('components_with_dedicated_messaging', []) %}
{%- set stacklight_enabled = spec.get('features', {}).get('stacklight', {}).get('enabled', False) %}
{%- set notification_topics = ['notifications'] %}
{%- do notification_topics.append('stacklight_notifications') if stacklight_enabled %}
{%- set neutron_backend = spec.features.neutron.get('backend', 'ml2') %}
{%- set baremetal_enabled = 'baremetal' in spec.features.services %}

spec:
  releases:
{%- if 'compute' in components_with_dedicated_messaging %}
  - name: openstack-nova-rabbitmq
    chart: {{spec.common.infra.repo}}/rabbitmq
    values:
{% include 'base/_rabbitmq_images.yaml' %}
  {%- if stacklight_enabled %}
      monitoring:
        prometheus:
          enabled: true
  {%- endif %}
      pod:
        replicas:
          server: 1
      manifests:
        network_policy: false
        job_users_create: true
      volume:
        enabled: false
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_messaging_dedicated.yaml' %}
      conf:
        users:
          nova:
            auth:
              service_user:
                username: {{ credentials.messaging.user.username }}
                password: {{ credentials.messaging.user.password }}
            path: /nova
        aux_conf:
          policies:
          - vhost: nova
            name: default-policy
            pattern: '^(?!amq\.).*'
            definition:
              message-ttl: 120000
          - vhost: nova
            name: results_expire
            pattern: '^results\.'
            definition:
              expires: 3600000
            priority: 1
          - vhost: nova
            name: tasks_expire
            pattern: '^tasks\.'
            definition:
              expires: 3600000
            priority: 1
  {%- if stacklight_enabled %}
        prometheus_exporter:
          rabbit_exporters: "overview,exchange,node"
  {%- endif %}
{%- endif %}
  - name: openstack-libvirt
    chart: {{spec.common.infra.repo}}/libvirt
    values:
      images:
        tags:
{%- for image in ["image_repo_sync",
                 "libvirt",
                 "ceph_config_helper",
                 "dep_check",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      manifests:
        network_policy: false
{%- if (spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'ceph') or (spec.get('features', {}).get('cinder', {}).get('volume', {}).get('backend') == 'ceph') %}
      ceph_client:
        configmap: rook-ceph-config
        user_secret_name: {{ ceph.nova.secrets }}
{%- endif %}
      conf:
        ceph:
{%- if (spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'ceph') or (spec.get('features', {}).get('cinder', {}).get('volume', {}).get('backend') == 'ceph') %}
          enabled: true
          cinder:
            user: {{ ceph.nova.username }}
            keyring: {{ ceph.nova.keyring }}
{%- else %}
          enabled: false
{%- endif %}
        libvirt:
          listen_addr: 0.0.0.0
      {%- if neutron_backend == 'tungstenfabric' %}
      network:
        backend: []
      {%- endif %}
{%- if spec.get('migration', {}).get('nova', {}).get('deploy_main_service', True) %}
  - name: openstack-nova
    chart: {{spec.common.openstack.repo}}/nova
    values:
      images:
        tags:
{%- for image in [
    "nova_cell_setup_init",
    "nova_placement",
    "nova_compute_ironic",
    "nova_db_sync",
    "nova_db_sync_online",
    "nova_db_sync_db",
    "nova_db_sync_api",
    "db_drop",
    "bootstrap",
    "image_repo_sync",
    "nova_compute_ssh",
    "ks_endpoints",
    "nova_api",
    "db_init",
    "nova_conductor",
    "dep_check",
    "nova_compute",
    "nova_novncproxy",
    "ks_user",
    "ks_service",
    "nova_spiceproxy",
    "nova_scheduler",
    "nova_novncproxy_assets",
    "nova_spiceproxy_assets",
    "rabbit_init",
    "nova_cell_setup",
    "nova_consoleauth",
    "test",
    "nova_service_cleaner",] %}
        {%- if image in images %}
          {{ image }}: {{ images[image] }}
        {%- endif %}
{%- endfor %}
      labels:
        agent:
          compute_ironic:
            node_selector_key: openstack-control-plane
            node_selector_value: enabled
      pod:
        # NOTE(ohryhorov): use_fqdn is disabled not to use FQDN
        # in service hostnames
        use_fqdn:
          compute: false
        # NOTE(vsaienko): don't use host networking to be able to
        # update pods with surge (when node hostport is used)
        useHostNetwork:
          novncproxy: false
        probes:
          rpc_timeout: 30
          rpc_retries: 2
          compute:
            default:
              liveness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
              readiness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
          conductor:
            default:
              liveness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
              readiness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
          consoleauth:
            default:
              liveness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
              readiness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
          novncproxy:
            default:
              liveness:
                enabled: True
                params:
                  initialDelaySeconds: 0
              readiness:
                enabled: True
                params:
                  initialDelaySeconds: 0
          scheduler:
            default:
              liveness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
              readiness:
                enabled: True
                params:
                  initialDelaySeconds: 0
                  periodSeconds: 50
                  timeoutSeconds: 40
        replicas:
          osapi: 3
          placement: 3
      dependencies:
        static:
          db_init:
            jobs:
              - openstack-mariadb-cluster-wait
      manifests:
        network_policy: false
        job_rabbit_init: false
{%- if spec.openstack_version not in ['queens', 'rocky'] %}
        deployment_consoleauth: false
# Since Rocky placemnt is deployed as separate service
        deployment_placement: false
        ingress_placement: false
        job_db_init_placement: false
        job_ks_placement_endpoints: false
        job_ks_placement_service: false
        job_ks_placement_user: false
        pdb_placement: false
        secret_keystone_placement: false
        service_ingress_placement: false
        service_placement: false
{%- endif %}
        secret_ca_bundle: true
      {%- if baremetal_enabled %}
        statefulset_compute_ironic: true
      {%- endif %}
      endpoints:
        cluster_domain_suffix: {{ spec.internal_domain_name }}
{% include 'base/_admin_identity.yaml' %}
{% include 'base/_cache.yaml' %}
        oslo_db:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            nova:
              username: {{ credentials.database.user.username }}
              password: {{ credentials.database.user.password }}
        oslo_db_api:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            nova:
              username: {{ credentials.database.user.username }}
              password: {{ credentials.database.user.password }}
        oslo_db_cell0:
          auth:
            admin:
              username: {{ admin_creds.database.username }}
              password: {{ admin_creds.database.password }}
            nova:
              username: {{ credentials.database.user.username }}
              password: {{ credentials.database.user.password }}
{%- if 'compute' in components_with_dedicated_messaging %}
{% include 'base/_messaging_dedicated.yaml' %}
{%- else %}
{% include 'base/_messaging_shared.yaml' %}
{%- endif %}
{% include 'base/_notifications.yaml' %}
        compute:
          host_fqdn_override:
            public:
              host: nova.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: nova-api
            default: nova
            internal: nova-api
            public:
              host: nova
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8774
              default: 80
              internal: 8774
              public: 443
          scheme:
            default: http
            public: https
        placement:
          host_fqdn_override:
            public:
              host: placement.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          hosts:
            admin:
              host: placement-api
            default: placement
            internal: placement-api
            public:
              host: placement
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent( width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent( width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent( width=18, first=True) }}
          port:
            api:
              admin: 8778
              default: 80
              internal: 8778
              public: 443
          scheme:
            default: http
            public: https
        compute_novnc_proxy:
          host_fqdn_override:
            public:
              host: novncproxy.{{ spec.public_domain_name }}
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent(width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent(width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent(width=18, first=True) }}
          hosts:
            default: nova-novncproxy
            public:
              host: novncproxy
              tls:
                ca: |
{{ spec.features.ssl.public_endpoints.ca_cert | indent(width=18, first=True) }}
                crt: |
{{ spec.features.ssl.public_endpoints.api_cert | indent(width=18, first=True) }}
                key: |
{{ spec.features.ssl.public_endpoints.api_key | indent(width=18, first=True) }}
          port:
            novnc_proxy:
              default: 6080
              public: 443
          scheme:
            default: http
            public: https
{%- if spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'ceph' or (spec.get('features', {}).get('cinder', {}).get('volume', {}).get('backend') == 'ceph') %}
      ceph_client:
        configmap: rook-ceph-config
        user_secret_name: {{ ceph.nova.secrets }}
{%- endif %}
      conf:
        ceph:
{%- if spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'ceph' %}
          enabled: true
          cinder:
            user: {{ ceph.nova.username }}
            keyring: {{ ceph.nova.keyring }}
{%- elif spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'local' %}
          enabled: false
{%- endif %}
        nova:
          DEFAULT:
            allow_resize_to_same_host: true
          cache:
            backend: oslo_cache.memcache_pool
          keystone_authtoken:
            memcache_security_strategy: ENCRYPT
            memcache_secret_key: {{ credentials.memcached }}
          ironic:
            memcache_security_strategy: ENCRYPT
            memcache_secret_key: {{ credentials.memcached }}
          cinder:
            # TODO(vsaienko): remove service_name when queens support is dropped.
            catalog_info: volumev3:cinderv3:internalURL
          neutron:
            metadata_proxy_shared_secret: {{ metadata_secret }}
 {%- if 'metering' in services %}
          notifications:
            notify_on_state_change: vm_and_task_state
{%- endif %}
{%- if neutron_backend == 'tungstenfabric' %}
          compute:
            live_migration_wait_for_vif_plug: false
          workarounds:
            skip_migration_revert_events: true
{%- endif %}
          libvirt:
            cpu_mode: custom
            cpu_model: kvm64
            virt_type: kvm
            rbd_user: {{ ceph.nova.username }}
{%- if spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'ceph' %}
            images_type: rbd
{%- elif spec.get('features', {}).get('nova', {}).get('images', {}).get('backend') == 'local' %}
            images_type: qcow2
{%- endif %}
{%- for pools, pools_config in ceph.nova.pools.items() %}
            images_rbd_pool: {{ pools_config.name }}
{%- endfor %}
          oslo_messaging_notifications:
            topics: {{ notification_topics|join(',') }}
          service_user:
            send_service_user_token: true
      {%- if baremetal_enabled %}
        nova_ironic:
          DEFAULT:
            compute_driver: ironic.IronicDriver
            reserved_host_memory_mb: 0
            reserved_host_cpu: 0
            reserved_host_disk_mb: 0
          filter_scheduler:
            track_instance_changes: false
          scheduler:
            discover_hosts_in_cells_interval: 120
      {%- endif %}
        logging:
          logger_nova:
            level: {{ spec.get('features', {}).get('logging', {}).get('nova', {}).get('level', 'INFO') }}
            propagate: 0
        {%- if spec.get('features', {}).get('nova', {}).get('live_migration_interface') %}
        libvirt:
          live_migration_interface: {{ spec.features.nova.live_migration_interface }}
        hypervisor:
          host_interface: {{ spec.features.nova.live_migration_interface }}
        {%- endif %}
        ssh_private: |
          {{ ssh_credentials.private|indent(10)|trim }}
        ssh_public: |
          {{ ssh_credentials.public }}
{%- if spec.openstack_version not in ['queens', 'rocky', 'stein', 'train'] %}
        policy:
          os_compute_api:limits: "@"
          os_compute_api:limits:other_project: rule:system_reader_api
          os_compute_api:os-agents:create: rule:system_admin_api
          os_compute_api:os-agents:delete: rule:system_admin_api
          os_compute_api:os-agents:list: rule:system_reader_api
          os_compute_api:os-agents:update: rule:system_admin_api
          os_compute_api:os-attach-interfaces:create: rule:system_admin_or_owner
          os_compute_api:os-attach-interfaces:delete: rule:system_admin_or_owner
          os_compute_api:os-attach-interfaces:list: rule:system_or_project_reader
          os_compute_api:os-attach-interfaces:show: rule:system_or_project_reader
          os_compute_api:os-availability-zone:list: "@"
          os_compute_api:os-deferred-delete:force: rule:system_admin_or_owner
          os_compute_api:os-deferred-delete:restore: rule:system_admin_or_owner
          os_compute_api:os-hypervisors:list: rule:system_reader_api
          os_compute_api:os-hypervisors:list-detail: rule:system_reader_api
          os_compute_api:os-hypervisors:search: rule:system_reader_api
          os_compute_api:os-hypervisors:servers: rule:system_reader_api
          os_compute_api:os-hypervisors:show: rule:system_reader_api
          os_compute_api:os-hypervisors:statistics: rule:system_reader_api
          os_compute_api:os-hypervisors:uptime: rule:system_reader_api
          os_compute_api:os-instance-actions:list: rule:system_or_project_reader
          os_compute_api:os-instance-actions:show: rule:system_or_project_reader
          os_compute_api:os-instance-usage-audit-log:list: rule:system_reader_api
          os_compute_api:os-instance-usage-audit-log:show: rule:system_reader_api
          os_compute_api:os-security-groups:add: rule:system_admin_or_owner
          os_compute_api:os-security-groups:list: rule:system_or_project_reader
          os_compute_api:os-security-groups:remove: rule:system_admin_or_owner
          os_compute_api:os-server-password:clear: rule:system_admin_or_owner
          os_compute_api:os-server-password:show: rule:system_or_project_reader
          os_compute_api:os-services:delete: rule:system_admin_api
          os_compute_api:os-services:list: rule:system_reader_api
          os_compute_api:os-services:update: rule:system_admin_api
          os_compute_api:os-unrescue: rule:system_admin_or_owner
{%- endif %}
      network:
        core_plugin: {{ neutron_backend }}
        {%- if neutron_backend == 'tungstenfabric' %}
        backend: []
        vrouter_port: {{ vrouter_port }}
        {%- endif %}
        sshd:
          enabled: true
      jobs:
{% include 'base/_ks_jobs.yaml' %}
{%- endif %}
